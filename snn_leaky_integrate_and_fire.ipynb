{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiagoribeiro00/neuroscience-computational/blob/main/snn_leaky_integrate_and_fire.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Execute esta cÃ©lula no Google Colab para instalar Norse\n",
        "!pip install norse torch torchvision --q"
      ],
      "metadata": {
        "id": "ZkOX4EdgmLsN",
        "outputId": "15684559-8bf5-4e68-8061-994a97295c2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.5 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”\u001b[0m\u001b[90mâ•º\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.3/1.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for norse (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rede Neural com NeurÃ´nio LIF\n"
      ],
      "metadata": {
        "id": "5CkyiK9xmYpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from norse.torch import LIFParameters, LIFState, LIFCell\n",
        "\n",
        "# Define o dispositivo\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Baixa o dataset MNIST (dÃ­gitos manuscritos)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Lambda(lambda x: x * 32)  # aumenta a intensidade do input\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "qe9GiC3JmNqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definindo a Rede com LIFCell\n"
      ],
      "metadata": {
        "id": "TwxtakJ2meGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SNN_LIF_Model(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        # Camada densa de entrada\n",
        "        self.fc1 = nn.Linear(28*28, 128)\n",
        "\n",
        "        # NeurÃ´nio LIF (Leaky Integrate-and-Fire)\n",
        "        self.lif1 = LIFCell()\n",
        "\n",
        "        # Camada de saÃ­da\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        seq_length = 10  # NÃºmero de steps no tempo\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # Inicializa o estado do neurÃ´nio LIF\n",
        "        lif_state = None\n",
        "\n",
        "        # Armazena os spikes ao longo do tempo\n",
        "        outputs = torch.zeros(batch_size, 10, device=x.device)\n",
        "\n",
        "        # Entrada deve ser achatada para vetor (imagem 28x28 â†’ vetor de 784)\n",
        "        x = x.view(batch_size, -1)\n",
        "\n",
        "        for t in range(seq_length):\n",
        "            z = self.fc1(x)\n",
        "\n",
        "            # Passa pelo neurÃ´nio LIF (retorna spike e novo estado)\n",
        "            s, lif_state = self.lif1(z, lif_state)\n",
        "\n",
        "            # Passa pela camada de saÃ­da\n",
        "            out = self.fc2(s)\n",
        "\n",
        "            # Soma as saÃ­das ao longo do tempo\n",
        "            outputs += out\n",
        "\n",
        "        return outputs / seq_length  # MÃ©dia das ativaÃ§Ãµes\n"
      ],
      "metadata": {
        "id": "1gynlAdemb53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Treinando o Modelo"
      ],
      "metadata": {
        "id": "HnUEQDe9mjvc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SNN_LIF_Model().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 5  # Pode aumentar para 5 ou 10 se quiser\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1} - Loss: {running_loss/len(trainloader):.4f}\")\n"
      ],
      "metadata": {
        "id": "sPUNGB8Imh4Q",
        "outputId": "da0722d6-5c51-4e3d-dc53-cadc087aa06a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 0.3055\n",
            "Epoch 2 - Loss: 0.1679\n",
            "Epoch 3 - Loss: 0.1344\n",
            "Epoch 4 - Loss: 0.1144\n",
            "Epoch 5 - Loss: 0.1029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AvaliaÃ§Ã£o da acurÃ¡cia no conjunto de treino\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in trainloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f\"AcurÃ¡cia no conjunto de treino: {100 * correct / total:.2f}%\")\n"
      ],
      "metadata": {
        "id": "Az4HSTxbqRXk",
        "outputId": "fb016fe5-a2b0-4d7a-ddc3-589db4fda85d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AcurÃ¡cia no conjunto de treino: 97.47%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ğŸ§  Spiking Neural Network com NeurÃ´nios LIF â€” ExplicaÃ§Ã£o\n",
        "\n",
        "### ğŸ“Œ PrÃ©-processamento de dados (MNIST)\n",
        "Utilizamos o dataset **MNIST** (dÃ­gitos manuscritos), comum em tarefas de classificaÃ§Ã£o.\n",
        "\n",
        "As imagens foram **normalizadas e multiplicadas por 32** para simular correntes de entrada mais fortes aos neurÃ´nios.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ—ï¸ ConstruÃ§Ã£o do Modelo SNN com LIF\n",
        "\n",
        "Criamos uma rede neural com a seguinte estrutura:\n",
        "\n",
        "- ğŸ”¹ **Camada densa (Linear)** que transforma a imagem de entrada (28x28 = 784 pixels) em um vetor de 128 neurÃ´nios.\n",
        "- ğŸ”¹ **NeurÃ´nio LIF (`LIFCell`)**, que recebe essa entrada e gera *spikes* ao longo do tempo.\n",
        "- ğŸ”¹ **Camada de saÃ­da (Linear)** que converte os spikes acumulados em uma prediÃ§Ã£o de classe (0 a 9).\n",
        "\n",
        "A propagaÃ§Ã£o Ã© feita ao longo de **vÃ¡rias janelas temporais** (*time steps*), simulando a dinÃ¢mica de um neurÃ´nio biolÃ³gico.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ‹ï¸ Treinamento da Rede\n",
        "\n",
        "- âœ… Utilizamos `CrossEntropyLoss` para calcular o erro de classificaÃ§Ã£o.\n",
        "- âœ… O otimizador **Adam** ajusta os pesos da rede com base nos erros.\n",
        "- âœ… A rede Ã© treinada por **1 Ã©poca** (mas pode ser ajustado para mais).\n",
        "- âœ… O output da rede Ã© a **mÃ©dia das ativaÃ§Ãµes temporais**, simulando uma **taxa de disparo (firing rate)**.\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ” Conceitos Importantes Demonstrados\n",
        "\n",
        "| Conceito                | ExplicaÃ§Ã£o |\n",
        "|-------------------------|------------|\n",
        "| **LIF Neuron**          | Modelo que acumula corrente e dispara um *spike* quando o limiar Ã© atingido. ApÃ³s o disparo, o potencial de membrana Ã© resetado. |\n",
        "| **Spikes ao longo do tempo** | A rede simula como os neurÃ´nios se comportam em diferentes instantes, processando entradas dinÃ¢micas. |\n",
        "| **SNN vs ANN**          | Em vez de usar valores contÃ­nuos (ex: ReLU), usamos *spikes* binÃ¡rios (0 ou 1) ao longo do tempo. |\n",
        "| **Treinamento supervisionado** | Apesar da natureza esparsa dos *spikes*, usamos funÃ§Ãµes de perda tradicionais (como `CrossEntropy`) e **backpropagation**. |\n",
        "\n",
        "---\n",
        "\n",
        "### ğŸ§ª AplicaÃ§Ãµes PrÃ¡ticas da Arquitetura\n",
        "\n",
        "- ğŸ”¸ Reconhecimento de padrÃµes em tempo real  \n",
        "- ğŸ”¸ Dispositivos embarcados (baixo consumo de energia)  \n",
        "- ğŸ”¸ SimulaÃ§Ãµes neuromÃ³rficas  \n",
        "- ğŸ”¸ RobÃ³tica biolÃ³gica\n",
        "\n"
      ],
      "metadata": {
        "id": "-dk8-KO5onAA"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uPmljpmGokdN"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}